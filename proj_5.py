# -*- coding: utf-8 -*-
"""proj 5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gyIGlzNE2OwDk6WY4ru7b513i9hZs7te
"""

import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical

from sklearn.metrics import confusion_matrix, classification_report

import matplotlib.pyplot as plt

def plot_training_metrics(history):
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    # Plotting accuracy
    axes[0].plot(history.history['accuracy'], label='Training Accuracy')
    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')
    axes[0].set_title('Training and Validation Accuracy')
    axes[0].set_xlabel('Epochs')
    axes[0].set_ylabel('Accuracy')
    axes[0].legend()

    # Plotting loss
    axes[1].plot(history.history['loss'], label='Training Loss')
    axes[1].plot(history.history['val_loss'], label='Validation Loss')
    axes[1].set_title('Training and Validation Loss')
    axes[1].set_xlabel('Epochs')
    axes[1].set_ylabel('Loss')
    axes[1].legend()

    plt.tight_layout()
    plt.show()

import seaborn as sns
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(y_true, y_pred, classes, cmap=plt.cm.Blues):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='g', cmap=cmap,
                xticklabels=classes, yticklabels=classes)
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.title('Confusion Matrix')
    plt.show()

(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# Normalize pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0

# One-hot encoding the labels
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

from keras.regularizers import l2
# PHASE 1
# ADDED REGULARIZATION BECAUSE MODEL WAS OVERFITTING
model1 = models.Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3), kernel_regularizer=l2(0.01)),
    Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(0.01)),
    Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(0.01)),
    Flatten(),
    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
    Dense(10, activation='softmax')
])

model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history1 = model1.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))



plot_training_metrics(history1)

test_pred = np.argmax(model1.predict(test_images), axis=1)
test_true = np.argmax(test_labels, axis=1)
classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
plot_confusion_matrix(test_true, test_pred, classes)

# PHASE 2
model2 = models.Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),
    MaxPooling2D((2,2)),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D((2,2)),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D((2,2)),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history2 = model2.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))


plot_training_metrics(history2)

test_pred = np.argmax(model2.predict(test_images), axis=1)
test_true = np.argmax(test_labels, axis=1)
classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
plot_confusion_matrix(test_true, test_pred, classes)

# PHASE 3
model3 = models.Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),
    MaxPooling2D((2,2)),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D((2,2)),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D((2,2)),
    Dropout(0.5),
    Flatten(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])
model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history3 = model3.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))
plot_training_metrics(history3)
test_pred3 = np.argmax(model3.predict(test_images), axis=1)
plot_confusion_matrix(test_true, test_pred3, classes)

# PHASE 4
# USED MOBILENET AS AN ALTERNATIVE TO SHUFFLE NET
base_model = tf.keras.applications.MobileNetV2(input_shape=(32,32,3), include_top=False, weights='imagenet')
for layer in base_model.layers[:-4]:  # Making all layers untrainable except the last 4
    layer.trainable = False

model4 = models.Sequential([
    base_model,
    Flatten(),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

model4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history4 = model4.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))

plot_training_metrics(history4)

test_pred = np.argmax(model4.predict(test_images), axis=1)
test_true = np.argmax(test_labels, axis=1)
classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
plot_confusion_matrix(test_true, test_pred, classes)